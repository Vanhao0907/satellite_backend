"""
å«æ˜Ÿèµ„æºè°ƒåº¦æœåŠ¡ - ä»…QVé¢‘æ®µç‰ˆæœ¬ï¼ˆæ”¯æŒå›¾ç‰‡å¯¼å‡º + ç®—æ³•é€‰æ‹© + æ•°æ®é›†ç»Ÿè®¡ + ZIPä¸‹è½½ï¼‰
ä¸²è”æ‰€æœ‰å¤„ç†æ­¥éª¤ï¼šæ•°æ®é›†æ„å»º â†’ ç»Ÿè®¡ä¿¡æ¯ â†’ å‹ç¼©ZIP â†’ è°ƒåº¦ç®—æ³• â†’ ç»“æœåˆå¹¶ â†’ å¯è§†åŒ–ï¼ˆHTML + å›¾ç‰‡ï¼‰
"""
import os
import shutil
import time
import logging
import zipfile
import csv
from datetime import datetime
from flask import current_app

from core.dataset_builder import DatasetBuilder
from core.scheduling_algorithm import SchedulingAlgorithm
from core.result_combiner import ResultCombiner
from core.gantt_chart_generator import GanttChartGenerator
from core.satisfaction_chart_generator import SatisfactionChartGenerator
from core.dataset_statistics import DatasetStatistics

logger = logging.getLogger(__name__)


class SchedulingService:
    """
    å«æ˜Ÿèµ„æºè°ƒåº¦æœåŠ¡ - ä»…QVé¢‘æ®µï¼ˆæ”¯æŒHTML + å›¾ç‰‡å¯¼å‡º + ç®—æ³•é€‰æ‹© + ç»Ÿè®¡ + ZIPä¸‹è½½ï¼‰
    è´Ÿè´£åè°ƒæ•´ä¸ªè°ƒåº¦æµç¨‹
    """

    def __init__(self, params):
        """
        åˆå§‹åŒ–æœåŠ¡

        Args:
            params: dict {
                "arc_data": str,
                "antenna_num": dict,
                "strategy": str,
                "time_window": int
            }
        """
        self.params = params
        self.task_id = self._generate_task_id()

        # ä»Flaské…ç½®ä¸­è·å–ç›®å½•è·¯å¾„
        self.raw_data_dir = os.path.join(
            current_app.config['RAW_DATA_DIR'],
            params['arc_data']
        )
        self.work_dir = os.path.join(
            current_app.config['TEMP_DATA_DIR'],
            self.task_id
        )

        # å­ç›®å½•å®šä¹‰
        self.dataset_dir = os.path.join(self.work_dir, 'dataset')
        self.output_dir = os.path.join(self.work_dir, 'output')
        self.result_dir = os.path.join(self.work_dir, 'result')
        self.charts_dir = os.path.join(self.work_dir, 'charts')

        # éªŒè¯åŸå§‹æ•°æ®æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.raw_data_dir):
            raise FileNotFoundError(
                f"æ•°æ®é›†ä¸å­˜åœ¨: {params['arc_data']} "
                f"(è·¯å¾„: {self.raw_data_dir})"
            )

        # åˆ›å»ºå·¥ä½œç›®å½•
        self._create_directories()

        logger.info(f"[{self.task_id}] æœåŠ¡åˆå§‹åŒ–å®Œæˆ (ä»…QVé¢‘æ®µ)")
        logger.info(f"[{self.task_id}] åŸå§‹æ•°æ®: {self.raw_data_dir}")
        logger.info(f"[{self.task_id}] å·¥ä½œç›®å½•: {self.work_dir}")
        logger.info(f"[{self.task_id}] ç«™ç‚¹æ•°: {len(params['antenna_num'])}")
        logger.info(f"[{self.task_id}] æ€»å¤©çº¿æ•°: {sum(params['antenna_num'].values())}")
        logger.info(f"[{self.task_id}] ç®—æ³•ç­–ç•¥: {params['strategy']}")
        logger.info(f"[{self.task_id}] æ—¶é—´çª—å£: {params['time_window']}ç§’")

    def execute(self):
        """
        æ‰§è¡Œå®Œæ•´çš„è°ƒåº¦æµç¨‹ï¼ˆæ”¯æŒå›¾ç‰‡å¯¼å‡ºã€ç®—æ³•é€‰æ‹©ã€ç»Ÿè®¡ä¿¡æ¯ã€ZIPä¸‹è½½ï¼‰

        Returns:
            dict: {
                "task_id": str,
                "elapsed_time": float,
                "statistics": dict (åŒ…å«ç«™ç‚¹ç»Ÿè®¡å’Œå«æ˜Ÿç±»å‹ç»Ÿè®¡),
                "charts": {
                    "gantt_chart_html": str,
                    "gantt_chart_image_url": str,
                    "satisfaction_chart_html": str,
                    "satisfaction_chart_image_url": str
                },
                "dataset_zip_url": str,
                "validation": dict
            }
        """
        start_time = time.time()

        try:
            logger.info(f"[{self.task_id}] ========== å¼€å§‹æ‰§è¡Œè°ƒåº¦æµç¨‹ (ä»…QVé¢‘æ®µ) ==========")

            # æ­¥éª¤1: æ„å»ºæ•°æ®é›†
            dataset_path = self._step1_build_dataset()

            # æ­¥éª¤1.5: ç»Ÿè®¡æ•°æ®é›†ä¿¡æ¯
            dataset_stats = self._step1_5_calculate_statistics(dataset_path)

            # æ­¥éª¤1.6: å‹ç¼©æ•°æ®é›†ä¸ºZIPå¹¶å¢åŠ æ•°æ®é›†é¢„è§ˆ
            dataset_zip_url = self._step1_6_create_dataset_zip(dataset_path)
            csv_preview_md = self._generate_csv_preview(dataset_path)

            # æ­¥éª¤1.7: ç”Ÿæˆç®—æ³•é…ç½®æ–‡ä»¶
            self._step1_7_generate_algorithm_config(dataset_path)

            # æ­¥éª¤2: æ‰§è¡Œè°ƒåº¦ç®—æ³•
            excel_path, statistics = self._step2_run_scheduling(dataset_path)

            # æ­¥éª¤3: åˆå¹¶ç»“æœ
            result_dataset_path = self._step3_combine_results(dataset_path, excel_path)

            # æ­¥éª¤4: ç”Ÿæˆç”˜ç‰¹å›¾ï¼ˆHTML + å›¾ç‰‡ï¼‰
            gantt_html, gantt_image_url = self._step4_generate_gantt_chart(result_dataset_path)

            # æ­¥éª¤5: ç”Ÿæˆæ»¡è¶³åº¦åˆ†æå›¾ï¼ˆHTML + å›¾ç‰‡ï¼‰
            satisfaction_html, satisfaction_image_url = self._step5_generate_satisfaction_chart(result_dataset_path)

            # è®¡ç®—æ€»è€—æ—¶
            elapsed_time = time.time() - start_time

            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯ï¼ˆç®—æ³•ç»Ÿè®¡ + æ•°æ®é›†ç»Ÿè®¡ï¼‰
            combined_statistics = {**statistics, **dataset_stats}

            # ç»„è£…è¿”å›ç»“æœ
            result = {
                'task_id': self.task_id,
                'elapsed_time': round(elapsed_time, 2),
                'statistics': combined_statistics,
                'charts': {
                    'gantt_chart_html': gantt_html,
                    'gantt_chart_image_url': gantt_image_url,
                    'satisfaction_chart_html': satisfaction_html,
                    'satisfaction_chart_image_url': satisfaction_image_url
                },
                'dataset_zip_url': dataset_zip_url,
                'preview_markdown': csv_preview_md,
                'validation': statistics.get('validation', {
                    'no_overflow': True,
                    'no_overlap': True,
                    'message': 'éªŒè¯é€šè¿‡'
                })
            }

            logger.info(f"[{self.task_id}] ========== è°ƒåº¦æµç¨‹å®Œæˆ ==========")
            logger.info(f"[{self.task_id}] æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            logger.info(f"[{self.task_id}] æˆåŠŸç‡: {statistics.get('success_rate_all', 0):.2%}")
            logger.info(f"[{self.task_id}] ç”˜ç‰¹å›¾URL: {gantt_image_url}")
            logger.info(f"[{self.task_id}] æ»¡è¶³åº¦å›¾URL: {satisfaction_image_url}")
            logger.info(f"[{self.task_id}] æ•°æ®é›†ZIP URL: {dataset_zip_url}")
            logger.info(f"[{self.task_id}] æ•°æ®é›†é¢„è§ˆ: {csv_preview_md[:100]}...")
            logger.info(f"[{self.task_id}] ç«™ç‚¹æ•°æ®é‡: {dataset_stats['station_data_counts']}")
            logger.info(f"[{self.task_id}] å«æ˜Ÿç±»å‹ç»Ÿè®¡: {dataset_stats['satellite_type_counts']}")

            # å¯é€‰ï¼šè‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆä¿ç•™é™æ€å›¾ç‰‡å’ŒZIPï¼‰
            if current_app.config.get('AUTO_CLEANUP', False):
                self._cleanup()

            return result

        except Exception as e:
            logger.error(f"[{self.task_id}] æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
            # å¤±è´¥æ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup()
            raise

    def _step1_build_dataset(self):
        """æ­¥éª¤1: æ„å»ºæ•°æ®é›†"""
        logger.info(f"[{self.task_id}] ã€æ­¥éª¤1/7ã€‘æ„å»ºæ•°æ®é›† (ä»…QVé¢‘æ®µ)...")

        builder = DatasetBuilder(
            raw_data_dir=self.raw_data_dir,
            output_dir=self.dataset_dir,
            antenna_config=self.params['antenna_num']
        )

        dataset_path = builder.build()

        logger.info(f"[{self.task_id}] æ•°æ®é›†æ„å»ºå®Œæˆ: {dataset_path}")
        return dataset_path

    def _step1_5_calculate_statistics(self, dataset_path):
        """æ­¥éª¤1.5: ç»Ÿè®¡æ•°æ®é›†ä¿¡æ¯"""
        logger.info(f"[{self.task_id}] ã€æ­¥éª¤1.5/7ã€‘ç»Ÿè®¡æ•°æ®é›†ä¿¡æ¯...")

        stats_calculator = DatasetStatistics(
            dataset_dir=dataset_path,
            antenna_config=self.params['antenna_num']
        )

        dataset_stats = stats_calculator.calculate()

        logger.info(f"[{self.task_id}] æ•°æ®é›†ç»Ÿè®¡å®Œæˆ:")
        logger.info(f"[{self.task_id}]   ç«™ç‚¹æ•°æ®é‡: {dataset_stats['station_data_counts']}")
        logger.info(f"[{self.task_id}]   å«æ˜Ÿç±»å‹ç»Ÿè®¡: {dataset_stats['satellite_type_counts']}")
        logger.info(f"[{self.task_id}]   æ€»ä»»åŠ¡æ•°ï¼ˆå»é‡ï¼‰: {dataset_stats['total_unique_tasks']}")

        return dataset_stats

    def _step1_6_create_dataset_zip(self, dataset_path):
        """æ­¥éª¤1.6: å‹ç¼©æ•°æ®é›†ä¸ºZIPæ–‡ä»¶"""
        logger.info(f"[{self.task_id}] ã€æ­¥éª¤1.6/7ã€‘å‹ç¼©æ•°æ®é›†ä¸ºZIP...")

        try:
            # 1. ç›´æ¥è·å– Flask åº”ç”¨å®ä¾‹çœŸæ­£çš„é™æ€æ–‡ä»¶ç›®å½•
            static_dir = current_app.static_folder
            
            # é˜²å¾¡æ€§ä»£ç ï¼šå¦‚æœ Flask æ²¡é…ç½® static_folderï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ static
            if not static_dir:
                static_dir = os.path.join(current_app.root_path, 'static')

            # 2. æ‰“å°æ—¥å¿—éªŒè¯è·¯å¾„
            logger.info(f"[{self.task_id}] >>> è°ƒè¯•: Flaské™æ€ç›®å½•ç»å¯¹è·¯å¾„: {static_dir}")
            
            # 3. ç¡®ä¿ç›®å½•å­˜åœ¨
            if not os.path.exists(static_dir):
                os.makedirs(static_dir)

            # è·å–URLå‰ç¼€é…ç½® 
            server_url = current_app.config.get('SERVER_URL', 'http://172.16.1.84:5000')
            static_prefix = current_app.config.get('STATIC_URL_PREFIX', '/static')

            # ç”ŸæˆZIPæ–‡ä»¶å
            zip_filename = f"{self.task_id}_dataset.zip"
            zip_filepath = os.path.join(static_dir, zip_filename)

            logger.info(f"[{self.task_id}]   ç›®æ ‡ZIPä¿å­˜è·¯å¾„: {zip_filepath}")

            # åˆ›å»ºZIPæ–‡ä»¶
            with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # éå†datasetç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
                for root, dirs, files in os.walk(dataset_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼ˆä¿æŒç›®å½•ç»“æ„ï¼‰
                        arcname = os.path.relpath(file_path, dataset_path)
                        zipf.write(file_path, arcname)
                        
            # è·å–ZIPæ–‡ä»¶å¤§å°
            if os.path.exists(zip_filepath):
                 zip_size = os.path.getsize(zip_filepath)
                 zip_size_mb = zip_size / (1024 * 1024)
                 logger.info(f"[{self.task_id}] âœ“ ZIPæ–‡ä»¶å·²ç”Ÿæˆï¼Œå¤§å°: {zip_size_mb:.2f} MB")
            else:
                 logger.error(f"[{self.task_id}] Ã— ZIPæ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼Œæ–‡ä»¶æœªæ‰¾åˆ°")
                 return None

            # æ„å»ºä¸‹è½½URL
            base_url = server_url.rstrip('/')
            prefix = static_prefix.strip('/')
            zip_url = f"{base_url}/{prefix}/{zip_filename}"

            logger.info(f"[{self.task_id}]   ä¸‹è½½URL: {zip_url}")

            return zip_url

        except Exception as e:
            logger.error(f"[{self.task_id}] ZIPåˆ›å»ºå¤±è´¥: {e}", exc_info=True)
            return None

    def _step1_7_generate_algorithm_config(self, dataset_path):
        """æ­¥éª¤1.7: ç”Ÿæˆç®—æ³•é…ç½®æ–‡ä»¶ï¼ˆæ”¯æŒç®—æ³•é€‰æ‹©ï¼‰"""
        logger.info(f"[{self.task_id}] ã€æ­¥éª¤1.7/7ã€‘ç”Ÿæˆç®—æ³•é…ç½®æ–‡ä»¶...")

        try:
            project_root = os.getcwd()
            config_dir = os.path.join(project_root, 'core', 'scheduling')
            config_path = os.path.join(config_dir, 'config.py')

            logger.info(f"[{self.task_id}]   é¡¹ç›®æ ¹ç›®å½•: {project_root}")
            logger.info(f"[{self.task_id}]   ç›®æ ‡é…ç½®ç›®å½•: {config_dir}")
            logger.info(f"[{self.task_id}]   ç›®æ ‡é…ç½®æ–‡ä»¶: {config_path}")

            if not os.path.exists(config_dir):
                logger.info(f"[{self.task_id}]   åˆ›å»ºé…ç½®ç›®å½•...")
                os.makedirs(config_dir, exist_ok=True)
                logger.info(f"[{self.task_id}]   âœ“ é…ç½®ç›®å½•å·²åˆ›å»º")
            else:
                logger.info(f"[{self.task_id}]   âœ“ é…ç½®ç›®å½•å·²å­˜åœ¨")

            root_folder = os.path.join(dataset_path, 'QV')
            time_window = self.params['time_window']
            strategy = self.params['strategy']

            # ========== åŠŸèƒ½ä¸€ï¼šæ ¹æ®ç­–ç•¥é€‰æ‹©METHOD ==========
            if strategy == "ä¼˜å…ˆçº§é©±åŠ¨å¼èµ„æºè°ƒåº¦ç®—æ³•":
                method_value = 3
                logger.info(f"[{self.task_id}]   ç®—æ³•ç­–ç•¥: ä¼˜å…ˆçº§é©±åŠ¨å¼ (METHOD=3)")
            elif strategy == "GRUæ¨¡æ‹Ÿé€€ç«ç®—æ³•":
                method_value = 2
                logger.info(f"[{self.task_id}]   ç®—æ³•ç­–ç•¥: GRUæ¨¡æ‹Ÿé€€ç« (METHOD=2)")
            else:
                # é»˜è®¤å€¼
                method_value = 3
                logger.warning(f"[{self.task_id}]   æœªçŸ¥ç­–ç•¥'{strategy}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ METHOD=3")

            config_content = f"""# ç®—æ³•é…ç½®æ–‡ä»¶ - è‡ªåŠ¨ç”Ÿæˆ
# ä»»åŠ¡ID: {self.task_id}
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# ç®—æ³•ç­–ç•¥: {strategy}

ROOT_FOLDER = r'{root_folder}'
OPTIMIZATION = 'TRUE'
METHOD = {method_value}
ANSWER_TYPE = 'TRUE'
TASK_INTERVAL = {time_window}
USE_SA = 'FALSE'
SA_MAX_TIME = 300

INTRA_STATION_BALANCE = 'FALSE'
ANTENNA_LOAD_METHOD = 'B'
LOAD_WEIGHT_TASK = 0.3
LOAD_WEIGHT_TIME = 0.7
"""

            logger.info(f"[{self.task_id}]   æ­£åœ¨å†™å…¥é…ç½®æ–‡ä»¶...")

            with open(config_path, 'w', encoding='utf-8') as f:
                f.write(config_content)

            logger.info(f"[{self.task_id}]   âœ“ æ–‡ä»¶å†™å…¥å®Œæˆ")

            if os.path.exists(config_path):
                file_size = os.path.getsize(config_path)
                logger.info(f"[{self.task_id}]   âœ“ éªŒè¯é€šè¿‡: æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å° {file_size} å­—èŠ‚")
                logger.info(f"[{self.task_id}] âœ“ ç®—æ³•é…ç½®æ–‡ä»¶ç”ŸæˆæˆåŠŸ!")
                self.algorithm_config_path = config_path
            else:
                error_msg = f"é…ç½®æ–‡ä»¶å†™å…¥åä»ä¸å­˜åœ¨: {config_path}"
                logger.error(f"[{self.task_id}]   âœ— {error_msg}")
                raise FileNotFoundError(error_msg)

        except Exception as e:
            logger.error(f"[{self.task_id}] âœ— ç”Ÿæˆé…ç½®æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            raise RuntimeError(f"æ— æ³•ç”Ÿæˆç®—æ³•é…ç½®æ–‡ä»¶: {e}") from e

    def _step2_run_scheduling(self, dataset_path):
        """æ­¥éª¤2: æ‰§è¡Œè°ƒåº¦ç®—æ³•"""
        logger.info(f"[{self.task_id}] ã€æ­¥éª¤2/7ã€‘æ‰§è¡Œè°ƒåº¦ç®—æ³•...")

        scheduler = SchedulingAlgorithm(
            dataset_dir=dataset_path,
            output_dir=self.output_dir,
            time_window=self.params['time_window']
        )

        excel_path, statistics = scheduler.run()

        logger.info(
            f"[{self.task_id}] è°ƒåº¦å®Œæˆ: "
            f"æˆåŠŸç‡={statistics.get('success_rate_all', 0):.2%}, "
            f"è´Ÿè½½æ ‡å‡†å·®={statistics.get('load_std', 0):.4f}"
        )

        return excel_path, statistics

    def _step3_combine_results(self, dataset_path, excel_path):
        """æ­¥éª¤3: åˆå¹¶ç»“æœ"""
        logger.info(f"[{self.task_id}] ã€æ­¥éª¤3/7ã€‘åˆå¹¶ç»“æœæ•°æ®...")

        combiner = ResultCombiner(
            dataset_dir=dataset_path,
            excel_path=excel_path,
            output_dir=self.result_dir
        )

        result_dataset_path = combiner.combine()

        logger.info(f"[{self.task_id}] ç»“æœåˆå¹¶å®Œæˆ: {result_dataset_path}")
        return result_dataset_path

    def _step4_generate_gantt_chart(self, result_dataset_path):
        """
        æ­¥éª¤4: ç”Ÿæˆç”˜ç‰¹å›¾ï¼ˆHTML + å›¾ç‰‡ï¼‰

        Returns:
            tuple: (html_content, image_url)
        """
        logger.info(f"[{self.task_id}] ã€æ­¥éª¤4/7ã€‘ç”Ÿæˆç”˜ç‰¹å›¾ï¼ˆHTML + å›¾ç‰‡ï¼‰...")

        generator = GanttChartGenerator(
            result_dir=result_dataset_path,
            output_dir=self.charts_dir
        )

        gantt_html, gantt_image_url = generator.generate(self.task_id)

        logger.info(f"[{self.task_id}] ç”˜ç‰¹å›¾ç”Ÿæˆå®Œæˆ")
        logger.info(f"[{self.task_id}]   HTMLé•¿åº¦: {len(gantt_html)} å­—ç¬¦")
        logger.info(f"[{self.task_id}]   å›¾ç‰‡URL: {gantt_image_url}")

        return gantt_html, gantt_image_url

    def _step5_generate_satisfaction_chart(self, result_dataset_path):
        """
        æ­¥éª¤5: ç”Ÿæˆæ»¡è¶³åº¦åˆ†æå›¾ï¼ˆHTML + å›¾ç‰‡ï¼‰

        Returns:
            tuple: (html_content, image_url)
        """
        logger.info(f"[{self.task_id}] ã€æ­¥éª¤5/7ã€‘ç”Ÿæˆæ»¡è¶³åº¦åˆ†æå›¾ï¼ˆHTML + å›¾ç‰‡ï¼‰...")

        generator = SatisfactionChartGenerator(
            result_dir=result_dataset_path,
            output_dir=self.charts_dir
        )

        satisfaction_html, satisfaction_image_url = generator.generate(self.task_id)

        logger.info(f"[{self.task_id}] æ»¡è¶³åº¦å›¾ç”Ÿæˆå®Œæˆ")
        logger.info(f"[{self.task_id}]   HTMLé•¿åº¦: {len(satisfaction_html)} å­—ç¬¦")
        logger.info(f"[{self.task_id}]   å›¾ç‰‡URL: {satisfaction_image_url}")

        return satisfaction_html, satisfaction_image_url

    def _generate_task_id(self):
        """ç”Ÿæˆä»»åŠ¡ID"""
        return f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        for directory in [self.dataset_dir, self.output_dir,
                         self.result_dir, self.charts_dir]:
            os.makedirs(directory, exist_ok=True)

    def _cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆä¿ç•™é™æ€å›¾ç‰‡å’ŒZIPï¼‰"""
        logger.info(f"[{self.task_id}] æ¸…ç†ä¸´æ—¶å·¥ä½œç›®å½•...")

        try:
            # æ¸…ç†ä¸´æ—¶å·¥ä½œç›®å½•
            if os.path.exists(self.work_dir):
                shutil.rmtree(self.work_dir)
                logger.info(f"[{self.task_id}] âœ“ ä¸´æ—¶å·¥ä½œç›®å½•å·²æ¸…ç†: {self.work_dir}")

            # æ¸…ç†ç®—æ³•é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            if hasattr(self, 'algorithm_config_path') and os.path.exists(self.algorithm_config_path):
                os.remove(self.algorithm_config_path)
                logger.info(f"[{self.task_id}] âœ“ ç®—æ³•é…ç½®æ–‡ä»¶å·²æ¸…ç†")

            # æ³¨æ„ï¼šé™æ€å›¾ç‰‡å’ŒZIPæ–‡ä»¶ä¿ç•™åœ¨ STATIC_FILES_DIR ä¸­
            logger.info(f"[{self.task_id}] â„¹ï¸  é™æ€æ–‡ä»¶ï¼ˆå›¾ç‰‡å’ŒZIPï¼‰å·²ä¿ç•™åœ¨é™æ€ç›®å½•")

        except Exception as e:
            logger.warning(f"[{self.task_id}] âš  æ¸…ç†å¤±è´¥: {str(e)}")

    def _generate_csv_preview(self, dataset_path):
        """
        è¾…åŠ©æ­¥éª¤: éå†æ•°æ®é›†ç›®å½•(é¢‘æ®µ->ç«™ç‚¹->CSV)ï¼Œç”Ÿæˆé¢„è§ˆè¡¨æ ¼
        ç»“æ„: dataset/QV/CM/CM01.csv
        """
        logger.info(f"[{self.task_id}] ç”ŸæˆCSVæ•°æ®é¢„è§ˆ...")
        preview_markdown = "###  æ•°æ®é›†æŠ½æ ·é¢„è§ˆ\n\n"
        
        try:
            # 1. è·å–ç¬¬ä¸€å±‚æ–‡ä»¶å¤¹ (é¢‘æ®µå±‚: QV, S)
            band_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]
            
            if not band_dirs:
                return "###  æ•°æ®é›†ç›®å½•ä¸ºç©º"

            # éå†æ¯ä¸ªé¢‘æ®µæ–‡ä»¶å¤¹ (QV, S)
            for band_name in band_dirs:
                band_path = os.path.join(dataset_path, band_name)
                
                # 2. è·å–ç¬¬äºŒå±‚æ–‡ä»¶å¤¹ (ç«™ç‚¹å±‚: CM, JMS, KEL...)
                station_dirs = [d for d in os.listdir(band_path) if os.path.isdir(os.path.join(band_path, d))]
                
                if not station_dirs:
                    continue

                # åœ¨Markdowné‡Œæ ‡æ˜é¢‘æ®µ
                preview_markdown += f"### ğŸ“¡ é¢‘æ®µ: {band_name}\n"

                # éå†æ¯ä¸ªç«™ç‚¹æ–‡ä»¶å¤¹
                for station_name in station_dirs:
                    station_path = os.path.join(band_path, station_name)
                    
                    # 3. å¯»æ‰¾ .csv æ–‡ä»¶
                    csv_files = [f for f in os.listdir(station_path) if f.endswith('.csv')]
                    
                    if not csv_files:
                        continue
                    
                    # éšæœºæˆ–å›ºå®šé€‰å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶
                    target_csv_name = csv_files[0]
                    target_csv_path = os.path.join(station_path, target_csv_name)
                    
                    # ç”Ÿæˆæ ‡é¢˜: ç«™ç‚¹å / æ–‡ä»¶å
                    preview_markdown += f"** ç«™ç‚¹: {station_name} / ğŸ“„ {target_csv_name}**\n"
                    
                    try:
                        # è¯»å– CSV
                        with open(target_csv_path, 'r', encoding='utf-8-sig') as f:
                            reader = csv.reader(f)
                            all_rows = list(reader)
                            
                            if not all_rows:
                                preview_markdown += "> *[æ–‡ä»¶ä¸ºç©º]*\n\n"
                                continue
                            
                            # è·å–è¡¨å¤´
                            header = all_rows[0]
                            # è·å–æ•°æ® (å–ç¬¬1åˆ°ç¬¬3è¡Œæ•°æ®ï¼Œå³ rows[1:4])
                            data_rows = all_rows[1:4] if len(all_rows) > 1 else []
                            
                            # --- ç”Ÿæˆè¡¨æ ¼ ---
                            # å†™å…¥è¡¨å¤´
                            preview_markdown += "| " + " | ".join(header) + " |\n"
                            # å†™å…¥åˆ†éš”çº¿
                            preview_markdown += "| " + " | ".join(["---"] * len(header)) + " |\n"
                            # å†™å…¥æ•°æ®è¡Œ
                            for row in data_rows:
                                preview_markdown += "| " + " | ".join(row) + " |\n"
                            
                            preview_markdown += "\n" # è¡¨æ ¼åç©ºä¸€è¡Œ
                            
                    except Exception as e:
                        preview_markdown += f"> *è¯»å–å‡ºé”™: {str(e)}*\n\n"

            return preview_markdown

        except Exception as e:
            logger.error(f"[{self.task_id}] ç”Ÿæˆé¢„è§ˆå¤±è´¥: {e}", exc_info=True)
            return "###  æ•°æ®é¢„è§ˆç”Ÿæˆå¤±è´¥"